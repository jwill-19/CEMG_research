{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "421d163d",
   "metadata": {},
   "source": [
    "# REF Simulation Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93211e5d",
   "metadata": {},
   "source": [
    "This is the code used to analyze a typical NetCDF track file dataset. The REF simulation is a historical simulation from 1985-2014. Several other datasets were also analyzed in a similar manner so those Jupyter Notebooks were not included in this repository. Those notebooks included analysis of IBTrACS (historical observation from 1985-2014), RCP4.5 (global warming scenario from 2070-2100), and RCP8.5 (stronger global warming scenario from 2070-2100). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec5f422",
   "metadata": {},
   "source": [
    "### Information for NetCDF File Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "086ac99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "todays_date = \"2021-09-06\"\n",
    "distance = \"200 km \"\n",
    "original_file = \"trajectories.CHEY.VR28.NATL.REF.CAM5.4CLM5.0.dtime900.003.nc\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1534ad9e",
   "metadata": {},
   "source": [
    "### Program to get information about a NetCDF file using Xarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d008d56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "\n",
    "track_file = \"trajectories.CHEY.VR28.NATL.REF.CAM5.4CLM5.0.dtime900.nc\" \n",
    "DS = xr.open_dataset(track_file) \n",
    "\n",
    "#print(DS)\n",
    "\n",
    "#print(DS.clon)\n",
    "#print(DS.clat)\n",
    "#print(DS.min_p)\n",
    "#print(DS.vmax_2D)\n",
    "#print(DS.time_str)\n",
    "#print(DS.seasons)\n",
    "\n",
    "#time = DS.time_str.values\n",
    "#print(time[1,0])\n",
    "#print(time[1,1])\n",
    "#print(time[1525,0])\n",
    "#year = DS.seasons.values\n",
    "#print(year)\n",
    "\n",
    "DS.close()\n",
    "\n",
    "#.nc total storms: 1526    // years: 1984-2014 (discard first storm for 1985)\n",
    "#002.nc total storms: 1536 // years: 1985-2014\n",
    "#003.nc total storms: 1444 // years: 1985-2014"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb7c88d",
   "metadata": {},
   "source": [
    "### Program to filter out North Atlantic TCs from Global Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3929a382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"trajectories.CHEY.VR28.NATL.REF.CAM5.4CLM5.0.dtime900.nc\" 1-1525\n",
    "# \"trajectories.CHEY.VR28.NATL.REF.CAM5.4CLM5.0.dtime900.002.nc\" all\n",
    "# \"trajectories.CORI.VR28.NATL.REF.CAM5.4CLM5.0.dtime900.003.nc\" all\n",
    "\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import LineString, Point\n",
    "from tc_analysis.tc_functions import *\n",
    "\n",
    "first_tc = 1\n",
    "last_tc = 1525\n",
    "\n",
    "original_file = \"trajectories.CHEY.VR28.NATL.REF.CAM5.4CLM5.0.dtime900.nc\" #open NetCDF file and extract values\n",
    "DS = xr.open_dataset(original_file) \n",
    "lons = DS.clon.sel(stormID=slice(first_tc, last_tc)).values                #only choose storms from 1985-2014\n",
    "lats = DS.clat.sel(stormID=slice(first_tc, last_tc)).values\n",
    "max_w = DS.vmax_2D.sel(stormID=slice(first_tc, last_tc)).values\n",
    "time = DS.time_str.sel(stormID=slice(first_tc, last_tc)).values\n",
    "DS.close() \n",
    "\n",
    "print(np.shape(lons))                            \n",
    "\n",
    "lons, lats, max_w, time = ref_na_filter(lons, lats, max_w, time)           #filter out non-North Atlantic TCs\n",
    "\n",
    "lons = change_lon_range(lons)      #change lon range to -180 to 180, also removes possible horizontal lines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312750ae",
   "metadata": {},
   "source": [
    "Make NetCDF file of Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962adb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ds = xr.Dataset(\n",
    "        data_vars = dict(\n",
    "        clon = ([\"stormID\", \"time\"], lons),\n",
    "        clat = ([\"stormID\", \"time\"], lats),\n",
    "        vmax_2D = ([\"stormID\", \"time\"], max_w),\n",
    "        time_str = ([\"stormID\", \"time\"], time)\n",
    "        ),\n",
    "        attrs = dict(\n",
    "        description = \"Simple NetCDF file with updated longitude, latitude, maximum wind speed, and times for all NA storms between 2070 and 2100 from the file \" + original_file,\n",
    "        author = \"Justin Willson\",\n",
    "        creation_date = todays_date\n",
    "        ),\n",
    ")\n",
    "\n",
    "print(new_ds)\n",
    "\n",
    "new_ds.to_netcdf(\"REF.NA.storms.nc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa618c2",
   "metadata": {},
   "source": [
    "### Program to find landfalling TCs and those that come within ___ km of the eastern US coast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2ee43af4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(398, 107)\n",
      "(111, 107)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import LineString, Point\n",
    "from tc_analysis.tc_functions import *\n",
    "\n",
    "track_file = \"REF003.NA.storms.nc\" #open NetCDF file and extract values\n",
    "DS = xr.open_dataset(track_file) \n",
    "lons = DS.clon.values             #only choose storms from 2070-2100\n",
    "lats = DS.clat.values\n",
    "max_w = DS.vmax_2D.values\n",
    "time = DS.time_str.values\n",
    "DS.close() \n",
    "\n",
    "shapefile1 = \"shapefiles/eastern_us.shp\"\n",
    "shapefile2 = \"shapefiles/eastern_us_300km.shp\"\n",
    "\n",
    "print(np.shape(lons))\n",
    "\n",
    "#lons1, lats1, max_w1, time1 = landfall_filter(lons, lats, max_w, time, shapefile1)  #get landfalling storms\n",
    "lons2, lats2, max_w2, time2 = landfall_filter(lons, lats, max_w, time, shapefile2)  #get storms within 100km of land"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d40c92c",
   "metadata": {},
   "source": [
    "Make NetCDF files of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b29a83e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset>\n",
      "Dimensions:   (stormID: 111, time: 107)\n",
      "Dimensions without coordinates: stormID, time\n",
      "Data variables:\n",
      "    clon      (stormID, time) float32 -84.447815 -83.232025 ... nan nan\n",
      "    clat      (stormID, time) float32 30.064241 30.36162 30.824593 ... nan nan\n",
      "    vmax_2D   (stormID, time) float32 16.27279 15.26212 12.32979 ... nan nan nan\n",
      "    time_str  (stormID, time) int32 1985061506 1985061512 ... -2147483647\n",
      "Attributes:\n",
      "    description:    Simple NetCDF file with updated longitude, latitude, maxi...\n",
      "    author:         Justin Willson\n",
      "    creation_date:  2021-07-27\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "new_ds1 = xr.Dataset(\n",
    "        data_vars = dict(\n",
    "        clon = ([\"stormID\", \"time\"], lons1),\n",
    "        clat = ([\"stormID\", \"time\"], lats1),\n",
    "        vmax_2D = ([\"stormID\", \"time\"], max_w1),\n",
    "        time_str = ([\"stormID\", \"time\"], time1)\n",
    "        ),\n",
    "        attrs = dict(\n",
    "        description = \"Simple NetCDF file with updated longitude, latitude, maximum wind speed, and times for all NA landfalling storms between 2070 and 2100 from the file \" + original_file,\n",
    "        author = \"Justin Willson\",\n",
    "        creation_date = todays_date\n",
    "        ),\n",
    ")\n",
    "\n",
    "print(new_ds1)\n",
    "\n",
    "new_ds1.to_netcdf(\"REF.NA.landfalling.storms.nc\")\n",
    "\"\"\"\n",
    "\n",
    "new_ds2 = xr.Dataset(\n",
    "        data_vars = dict(\n",
    "        clon = ([\"stormID\", \"time\"], lons2),\n",
    "        clat = ([\"stormID\", \"time\"], lats2),\n",
    "        vmax_2D = ([\"stormID\", \"time\"], max_w2),\n",
    "        time_str = ([\"stormID\", \"time\"], time2)\n",
    "        ),\n",
    "        attrs = dict(\n",
    "        description = \"Simple NetCDF file with updated longitude, latitude, maximum wind speed, and times for all NA storms that come within\" + distance + \"of the eastern US between 2070 and 2100 from the file \" + original_file,\n",
    "        author = \"Justin Willson\",\n",
    "        creation_date = todays_date\n",
    "        ),\n",
    ")\n",
    "\n",
    "print(new_ds2)\n",
    "\n",
    "new_ds2.to_netcdf(\"300km_analysis/REF003.NA.landfalling.storms.300km.nc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9a027f",
   "metadata": {},
   "source": [
    "### Program to find the points on land, in the ocean, and in the ___ km buffer for the storms that come within ___ km of the eastern US coast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d78ef4b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(98, 107)\n",
      "(98, 107)\n",
      "(98, 107)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import LineString, Point\n",
    "from tc_analysis.tc_functions import *\n",
    "\n",
    "track_file = \"200km_analysis/REF003.NA.landfalling.storms.200km.nc\"    #open NetCDF file and extract values\n",
    "DS = xr.open_dataset(track_file) \n",
    "lons = DS.clon.values              #only choose storms from 1985-2014\n",
    "lats = DS.clat.values\n",
    "max_w = DS.vmax_2D.values\n",
    "time = DS.time_str.values\n",
    "DS.close() \n",
    "\n",
    "shapefile1 = \"shapefiles/eastern_us.shp\"\n",
    "shapefile2 = \"shapefiles/eastern_us_200km.shp\"\n",
    "\n",
    "print(np.shape(lons))\n",
    "\n",
    "#lons1, lats1, max_w1, time1 = get_landfall_points(lons, lats, max_w, time, shapefile1)  #get points on land\n",
    "lons2, lats2, max_w2, time2 = get_ocean_points(lons, lats, max_w, time, shapefile1)     #get points outside of land\n",
    "lons3, lats3, max_w3, time3 = get_landfall_points(lons2, lats2, max_w2, time2, shapefile2) #get points in buffer\n",
    "lons3, lats3, max_w3, time3 = remove_unwanted_points(lons3, lats3, max_w3, time3)   #remove points not in Atlantic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824e35c2",
   "metadata": {},
   "source": [
    "Make NetCDF files of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2c78429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset>\n",
      "Dimensions:   (stormID: 98, time: 107)\n",
      "Dimensions without coordinates: stormID, time\n",
      "Data variables:\n",
      "    clon      (stormID, time) float32 -79.3074 -78.31302 -77.02615 ... nan nan\n",
      "    clat      (stormID, time) float32 31.225912 31.267113 31.506567 ... nan nan\n",
      "    vmax_2D   (stormID, time) float32 14.28621 19.68029 19.727 ... 0.0 0.0 0.0\n",
      "    time_str  (stormID, time) int32 1985061606 1985061612 1985061618 ... 0 0 0\n",
      "Attributes:\n",
      "    description:    Simple NetCDF file with updated longitude, latitude, maxi...\n",
      "    author:         Justin Willson\n",
      "    creation_date:  2021-07-28\n",
      "<xarray.Dataset>\n",
      "Dimensions:   (stormID: 98, time: 107)\n",
      "Dimensions without coordinates: stormID, time\n",
      "Data variables:\n",
      "    clon      (stormID, time) float32 -79.3074 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0\n",
      "    clat      (stormID, time) float32 31.225912 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0\n",
      "    vmax_2D   (stormID, time) float32 14.28621 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0\n",
      "    time_str  (stormID, time) int32 1985061606 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0\n",
      "Attributes:\n",
      "    description:    Simple NetCDF file with updated longitude, latitude, maxi...\n",
      "    author:         Justin Willson\n",
      "    creation_date:  2021-07-28\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "new_ds1 = xr.Dataset(\n",
    "        data_vars = dict(\n",
    "        clon = ([\"stormID\", \"time\"], lons1),\n",
    "        clat = ([\"stormID\", \"time\"], lats1),\n",
    "        vmax_2D = ([\"stormID\", \"time\"], max_w1),\n",
    "        time_str = ([\"stormID\", \"time\"], time1)\n",
    "        ),\n",
    "        attrs = dict(\n",
    "        description = \"Simple NetCDF file with updated longitude, latitude, maximum wind speed, and times for all storm points that are on land between 2070 and 2100 from the file \" + original_file,\n",
    "        author = \"Justin Willson\",\n",
    "        creation_date = todays_date\n",
    "        ),\n",
    ")\n",
    "\n",
    "print(new_ds1)\n",
    "\n",
    "new_ds1.to_netcdf(\"REF.NA.landfalling.storms.100km.land.pts.nc\")\n",
    "\"\"\"\n",
    "\n",
    "new_ds2 = xr.Dataset(\n",
    "        data_vars = dict(\n",
    "        clon = ([\"stormID\", \"time\"], lons2),\n",
    "        clat = ([\"stormID\", \"time\"], lats2),\n",
    "        vmax_2D = ([\"stormID\", \"time\"], max_w2),\n",
    "        time_str = ([\"stormID\", \"time\"], time2)\n",
    "        ),\n",
    "        attrs = dict(\n",
    "        description = \"Simple NetCDF file with updated longitude, latitude, maximum wind speed, and times for all storm points that are in the ocean between 2070 and 2100 from the file \" + original_file,\n",
    "        author = \"Justin Willson\",\n",
    "        creation_date = todays_date\n",
    "        ),\n",
    ")\n",
    "\n",
    "print(new_ds2)\n",
    "\n",
    "new_ds2.to_netcdf(\"200km_analysis/REF003.NA.landfalling.storms.200km.ocean.pts.nc\")\n",
    "\n",
    "new_ds3 = xr.Dataset(\n",
    "        data_vars = dict(\n",
    "        clon = ([\"stormID\", \"time\"], lons3),\n",
    "        clat = ([\"stormID\", \"time\"], lats3),\n",
    "        vmax_2D = ([\"stormID\", \"time\"], max_w3),\n",
    "        time_str = ([\"stormID\", \"time\"], time3)\n",
    "        ),\n",
    "        attrs = dict(\n",
    "        description = \"Simple NetCDF file with updated longitude, latitude, maximum wind speed, and times for all landfalling storms in the eastern U.S. between 2070 and 2100 from the file \" + original_file + \". Only points where the storm is within\" + distance + \"of land are included.\",\n",
    "        author = \"Justin Willson\",\n",
    "        creation_date = todays_date\n",
    "        ),\n",
    ")\n",
    "\n",
    "print(new_ds3)\n",
    "\n",
    "new_ds3.to_netcdf(\"200km_analysis/REF003.NA.landfalling.storms.200km.buffer.pts.nc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26556ec",
   "metadata": {},
   "source": [
    "### Program to make a quick plot of results to confirm accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ee94be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import LineString, Point\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "import matplotlib.patches as mpatches\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cf\n",
    "from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER\n",
    "from tc_analysis.tc_functions import *\n",
    "\n",
    "track_file = \"300km_analysis/REF003.NA.landfalling.storms.300km.buffer.pts.nc\"    #open NetCDF file and extract values\n",
    "DS = xr.open_dataset(track_file) \n",
    "lons = DS.clon.values                                            #only choose storms from 1985-2014\n",
    "lats = DS.clat.values\n",
    "max_w = DS.vmax_2D.values\n",
    "time = DS.time_str.values\n",
    "DS.close() \n",
    "\n",
    "gdf = gpd.read_file(\"shapefiles/eastern_us.shp\")                 #open shapefile and create geodataframe\n",
    "gdf = gdf.to_crs(\"EPSG:4326\")                                    #convert to lat and lon from platecarree\n",
    "\n",
    "plt.figure(figsize=(12,7))\n",
    "ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "ax.set_extent([-63, -115, 21, 55], crs=ccrs.PlateCarree())       #plot eastern U.S.\n",
    "ax.set_title('REF NA Basin 1985-2014', fontsize=20)\n",
    "gl = ax.gridlines(crs=ccrs.PlateCarree(), draw_labels=True, alpha=0)\n",
    "gl.xlabels_top = False\n",
    "#gl.ylabels_right = False\n",
    "#gl.xlocator = mticker.FixedLocator([-180, -45, 0, 45, 180])\n",
    "#gl.ylocator = mticker.FixedLocator([-90, -45, 0, 45, 90])\n",
    "gl.xformatter = LONGITUDE_FORMATTER\n",
    "gl.yformatter = LATITUDE_FORMATTER\n",
    "gl.xlabel_style = {'size': 14}\n",
    "gl.ylabel_style = {'size': 14}\n",
    "yellow_patch = mpatches.Patch(color='y', label='TD')    #create a legend\n",
    "green_patch = mpatches.Patch(color='g', label='TS') \n",
    "cyan_patch = mpatches.Patch(color='c', label='Cat1') \n",
    "blue_patch = mpatches.Patch(color='b', label='Cat2') \n",
    "red_patch = mpatches.Patch(color='r', label='Cat3') \n",
    "black_patch = mpatches.Patch(color='k', label='Cat4/5') \n",
    "plt.legend(handles=[yellow_patch, green_patch, cyan_patch, blue_patch, \n",
    "                    red_patch, black_patch], loc='upper left')\n",
    "\n",
    "gdf.plot(ax=ax, color='w', edgecolor='k')               #plot the shapefile \n",
    "\n",
    "#plot_tc_trajectories(lons, lats)                       #plot TCs \n",
    "plot_tc_points(lons, lats)\n",
    "\n",
    "#ref_plot_trajectories_byintensity(lons, lats, max_w, ax)   #plot TC trajectories by intensity\n",
    "#ref_plot_points_byintensity(lons, lats, max_w, ax)         #plot TC trajectory points by intensity\n",
    "\n",
    "plt.savefig(\"Aref300.png\")                                  #save figure in the same directory\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d3e2e1",
   "metadata": {},
   "source": [
    "### Program to create a histogram of all intensitites, avg intensity, and max intensity distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ca80b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import statistics\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import LineString, Point\n",
    "\n",
    "track_file = \"REF003.NA.landfalling.storms.100km.buffer.pts.nc\"     #open NetCDF file and extract values\n",
    "DS = xr.open_dataset(track_file) \n",
    "lons = DS.clon.values         #only choose storms from 1985-2014\n",
    "lats = DS.clat.values\n",
    "max_w = DS.vmax_2D.values\n",
    "time = DS.time_str.values\n",
    "DS.close()\n",
    "\n",
    "nstorms = np.shape(max_w)[0]  #get number of storms and times\n",
    "ntimes = np.shape(max_w)[1]\n",
    "\n",
    "max_list = []\n",
    "for i in range(nstorms):\n",
    "    max_int = max(max_w[i,:])\n",
    "    max_list.append(max_int)\n",
    "\n",
    "max_array = np.asarray(max_list)      #calculate the max wind speed for each storm's points in buffer region\n",
    "max_array = max_array[max_array > 0]  #remove 0 entries\n",
    "\n",
    "avg_list = []\n",
    "for i in range(nstorms-1):\n",
    "    k = 0\n",
    "    storm = max_w[i,:]\n",
    "    storm = storm[storm > 0]\n",
    "    if len(storm) == 0:\n",
    "        avg_int = 0\n",
    "    else:\n",
    "        avg_int = statistics.mean(storm)\n",
    "    avg_list.append(avg_int)\n",
    "\n",
    "avg_array = np.asarray(avg_list)      #calculate the avg wind speed for each storm's points in buffer region\n",
    "avg_array = avg_array[avg_array > 0]  #remove 0 entries\n",
    "\n",
    "total_list = []\n",
    "for i in range(nstorms):\n",
    "    for j in range(ntimes):\n",
    "        total_list.append(max_w[i,j])\n",
    "\n",
    "total_array = np.asarray(total_list)  #calculate the wind speed for each storm's points in buffer region\n",
    "total_array = total_array[total_array > 0] #remove 0 entries\n",
    "\n",
    "fig, ax = plt.subplots(3, 1, figsize=(7,10)) #plot these values in a 3 part histogram\n",
    "\n",
    "bin_intervals = np.arange(0, 100, 10)\n",
    "\n",
    "plot1 = ax[0].hist(total_array, bins=bin_intervals, edgecolor='k')\n",
    "ax[0].set_ylabel('Frequency')\n",
    "ax[0].set_title('REF003: Distribution of all Intensities (<= 100 km)')\n",
    "\n",
    "plot2 = ax[1].hist(avg_array, bins=bin_intervals, edgecolor='k')\n",
    "ax[1].set_ylabel('Frequency')\n",
    "ax[1].set_title('Distribution of Average Intensities (<= 100 km)')\n",
    "\n",
    "plot3 = ax[2].hist(max_array, bins=bin_intervals, edgecolor='k')\n",
    "ax[2].set_xlabel('Wind Speed (m/s)')\n",
    "ax[2].set_ylabel('Frequency')\n",
    "ax[2].set_title('Distribution of Maximum Intensities (<= 100 km)')\n",
    "\n",
    "plt.savefig(\"ref003_hist.png\")\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
