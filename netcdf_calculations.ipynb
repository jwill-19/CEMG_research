{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3af0223c",
   "metadata": {},
   "source": [
    "# NetCDF File Calulations and Manipulations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd12c2b",
   "metadata": {},
   "source": [
    "### Open NetCDF file, extract values, create new NetCDF file, print contents of said file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d1aaa71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset>\n",
      "Dimensions:   (stormID: 1526, time: 131)\n",
      "Dimensions without coordinates: stormID, time\n",
      "Data variables:\n",
      "    clon      (stormID, time) float32 ...\n",
      "    clat      (stormID, time) float32 ...\n",
      "    min_p     (stormID, time) float32 ...\n",
      "    vmax_2D   (stormID, time) float32 ...\n",
      "    time_str  (stormID, time) int32 ...\n",
      "    seasons   (stormID) float64 ...\n",
      "Attributes:\n",
      "    description:    Results from tempestextremes StitchNodes reformatted to N...\n",
      "    author:         A. M. Stansfield\n",
      "    creation_date:  2021-05-07\n"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "\n",
    "track_file = \"trajectories.CHEY.VR28.NATL.REF.CAM5.4CLM5.0.dtime900.nc\"\n",
    "DS = xr.open_dataset(track_file)\n",
    "print(DS)\n",
    "lons = DS.clon.values\n",
    "lats = DS.clat.values\n",
    "max_w = DS.vmax_2D.values\n",
    "time = DS.time_str.values\n",
    "DS.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65ac098f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset>\n",
      "Dimensions:   (stormID: 1526, time: 131)\n",
      "Dimensions without coordinates: stormID, time\n",
      "Data variables:\n",
      "    clon      (stormID, time) float32 111.72689 111.72689 110.57981 ... nan nan\n",
      "    clat      (stormID, time) float32 5.70914 5.70914 5.430289 ... nan nan nan\n",
      "    vmax_2D   (stormID, time) float32 13.81685 15.68618 21.85947 ... nan nan nan\n",
      "    time_str  (stormID, time) int32 1984122618 1984122700 ... -2147483647\n",
      "Attributes:\n",
      "    description:    Simple NetCDF file with updated longitude, latitude, maxi...\n",
      "    author:         Justin Willson\n",
      "    creation_date:  2021-07-07\n"
     ]
    }
   ],
   "source": [
    "new_ds = xr.Dataset(\n",
    "        data_vars = dict(\n",
    "        clon = ([\"stormID\", \"time\"], lons),\n",
    "        clat = ([\"stormID\", \"time\"], lats),\n",
    "        vmax_2D = ([\"stormID\", \"time\"], max_w),\n",
    "        time_str = ([\"stormID\", \"time\"], time)\n",
    "        ),\n",
    "        attrs = dict(\n",
    "        description = \"Simple NetCDF file with updated longitude, latitude, maximum wind speed, and times for all storms that fit a certain criteria\",\n",
    "        author = \"Justin Willson\",\n",
    "        creation_date = \"2021-07-08\"\n",
    "        ),\n",
    ")\n",
    "\n",
    "print(new_ds)\n",
    "\n",
    "#new_ds.to_netcdf(\"test.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58cb301",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = xr.open_dataset(\"test.nc\")\n",
    "print(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9525c2",
   "metadata": {},
   "source": [
    "### Program to find total storms and total data points in a TC track file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94ad5fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nstorms = 210\n",
      "Num data points = 1259\n",
      "95th percentile int (all) = 61.614412307739215\n",
      "95th percentile int (max) = 68.36997909545893\n",
      "95th percentile int (avg) = 50.92854245200988\n"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import statistics\n",
    "\n",
    "track_file1 = \"IBTrACS.NA.v04r00.19852014.storms.nc\"\n",
    "#track_file1 = \"100km_analysis/IBTrACS.NA.v04r00.landfalling.storms.100km.buffer.pts.nc\"\n",
    "track_file2 = \"300km_analysis/RCP85.COMB.NA.landfalling.storms.300km.buffer.pts.nc\"\n",
    "\n",
    "DS = xr.open_dataset(track_file2)\n",
    "max_w = DS.vmax_2D.values\n",
    "DS.close()\n",
    "\n",
    "nstorms = np.shape(max_w)[0]  #get number of storms and times\n",
    "ntimes = np.shape(max_w)[1]\n",
    "\n",
    "n = 0\n",
    "for i in range(nstorms):       #get number of storms excluding 0 entries\n",
    "    if sum(max_w[i,:]) == 0:\n",
    "        continue\n",
    "    else:\n",
    "        n = n+1\n",
    "\n",
    "new_nstorms = n\n",
    "print(\"nstorms = \" + str(new_nstorms))\n",
    "\n",
    "total_list = []\n",
    "for i in range(nstorms):\n",
    "    for j in range(ntimes):\n",
    "        if np.isnan == True:\n",
    "            continue\n",
    "        else:\n",
    "            total_list.append(max_w[i,j])\n",
    "\n",
    "total_array = np.asarray(total_list)       #calculate the wind speed for each storm's points in buffer region\n",
    "total_array = total_array[total_array > 0] #remove 0 entries\n",
    "#median_int_all = np.median(total_array)    #calculate median\n",
    "#median_int_all = np.median(total_array)* 0.51444444444444  #calculate median (if IBTrACS)\n",
    "print(\"Num data points = \" + str(len(total_array)))\n",
    "#print(\"median int (all) = \" + str(median_int_all))\n",
    "percentile95_int_all = np.percentile(total_array, 95)\n",
    "print(\"95th percentile int (all) = \" + str(percentile95_int_all))\n",
    "\n",
    "max_list = []\n",
    "for i in range(nstorms):\n",
    "    max_int = max(max_w[i,:])\n",
    "    max_list.append(max_int)\n",
    "\n",
    "max_array = np.asarray(max_list)          #calculate the max wind speed for each storm's points in buffer region\n",
    "max_array = max_array[max_array > 0]      #remove 0 entries\n",
    "#max_array = max_array * 0.51444444444444  #convert to m/s if file is IBTrACS\n",
    "#median_int_max = np.median(max_array)     #calculate median\n",
    "#print(\"median int (max) = \" + str(median_int_max))\n",
    "percentile95_int_max = np.percentile(max_array, 95)\n",
    "print(\"95th percentile int (max) = \" + str(percentile95_int_max))\n",
    "\n",
    "avg_list = []\n",
    "for i in range(nstorms):\n",
    "    k = 0\n",
    "    storm = max_w[i,:]\n",
    "    storm = storm[storm > 0]\n",
    "    if len(storm) == 0:\n",
    "        avg_int = 0\n",
    "    else:\n",
    "        avg_int = statistics.mean(storm)\n",
    "    avg_list.append(avg_int)\n",
    "\n",
    "avg_array = np.asarray(avg_list)          #calculate the avg wind speed for each storm's points in buffer region\n",
    "avg_array = avg_array[avg_array > 0]      #remove 0 entries\n",
    "#avg_array = avg_array * 0.51444444444444  #convert to m/s if file is IBTrACS\n",
    "#median_int_avg = np.median(avg_array)     #calculate median\n",
    "#print(\"median int (avg) = \" + str(median_int_avg))\n",
    "percentile95_int_avg = np.percentile(avg_array, 95)\n",
    "print(\"95th percentile int (avg) = \" + str(percentile95_int_avg))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3549d97f",
   "metadata": {},
   "source": [
    "### Program to calculate the percentage of storms above 50 m/s using a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93d51b72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4895608351331893\n",
      "2.601794016813037\n"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "from tc_analysis.tc_functions import *\n",
    "\n",
    "track_file1 = \"100km_analysis/IBTrACS.NA.v04r00.landfalling.storms.100km.buffer.pts.nc\"\n",
    "track_file2 = \"100km_analysis/REF.COMB.NA.landfalling.storms.100km.buffer.pts.nc\"\n",
    "track_file3 = \"100km_analysis/RCP45.COMB.NA.landfalling.storms.100km.buffer.pts.nc\"\n",
    "track_file4 = \"100km_analysis/RCP85.COMB.NA.landfalling.storms.100km.buffer.pts.nc\"\n",
    "\n",
    "track_file5 = \"200km_analysis/IBTrACS.NA.v04r00.landfalling.storms.200km.buffer.pts.nc\"\n",
    "track_file6 = \"200km_analysis/REF.COMB.NA.landfalling.storms.200km.buffer.pts.nc\"\n",
    "track_file7 = \"200km_analysis/RCP45.COMB.NA.landfalling.storms.200km.buffer.pts.nc\"\n",
    "track_file8 = \"200km_analysis/RCP85.COMB.NA.landfalling.storms.200km.buffer.pts.nc\"\n",
    "\n",
    "track_file9 = \"300km_analysis/IBTrACS.NA.v04r00.landfalling.storms.300km.buffer.pts.nc\"\n",
    "track_file10 = \"300km_analysis/REF.COMB.NA.landfalling.storms.300km.buffer.pts.nc\"\n",
    "track_file11 = \"300km_analysis/RCP45.COMB.NA.landfalling.storms.300km.buffer.pts.nc\"\n",
    "track_file12 = \"300km_analysis/RCP85.COMB.NA.landfalling.storms.300km.buffer.pts.nc\"\n",
    "\n",
    "track_file13 = \"IBTrACS.NA.v04r00.19852014.storms.nc\"\n",
    "track_file14 = \"REF003.NA.storms.nc\"\n",
    "\n",
    "DS = xr.open_dataset(track_file13)\n",
    "max_w1 = DS.vmax_2D.values* 0.51444444444444\n",
    "DS.close()\n",
    "\n",
    "DS = xr.open_dataset(track_file14)\n",
    "max_w = DS.vmax_2D.values\n",
    "DS.close()\n",
    "\n",
    "percent1 = intensity_prob(max_w1, 50.0, direction='greater')*100\n",
    "percent2 = intensity_prob(max_w, 50.0, direction='greater')*100\n",
    "print(percent1)\n",
    "print(percent2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778de13f",
   "metadata": {},
   "source": [
    "### Program to calculate the translation speed of the points in a specified file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd5aa3da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100km_analysis/IBTrACS.NA.v04r00.landfalling.storms.100km.buffer.pts.nc: 13.07\n",
      "100km_analysis/REF.COMB.NA.landfalling.storms.100km.buffer.pts.nc: 13.31\n",
      "100km_analysis/RCP45.COMB.NA.landfalling.storms.100km.buffer.pts.nc: 13.5\n",
      "100km_analysis/RCP85.COMB.NA.landfalling.storms.100km.buffer.pts.nc: 13.49\n",
      "200km_analysis/IBTrACS.NA.v04r00.landfalling.storms.200km.buffer.pts.nc: 14.87\n",
      "200km_analysis/REF.COMB.NA.landfalling.storms.200km.buffer.pts.nc: 14.68\n",
      "200km_analysis/RCP45.COMB.NA.landfalling.storms.200km.buffer.pts.nc: 15.59\n",
      "200km_analysis/RCP85.COMB.NA.landfalling.storms.200km.buffer.pts.nc: 15.25\n",
      "300km_analysis/IBTrACS.NA.v04r00.landfalling.storms.300km.buffer.pts.nc: 15.65\n",
      "300km_analysis/REF.COMB.NA.landfalling.storms.300km.buffer.pts.nc: 15.88\n",
      "300km_analysis/RCP45.COMB.NA.landfalling.storms.300km.buffer.pts.nc: 16.8\n",
      "300km_analysis/RCP85.COMB.NA.landfalling.storms.300km.buffer.pts.nc: 16.81\n"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import math\n",
    "from tc_analysis.tc_functions import *\n",
    "\n",
    "track_file1 = \"100km_analysis/IBTrACS.NA.v04r00.landfalling.storms.100km.buffer.pts.nc\"\n",
    "track_file2 = \"100km_analysis/REF.COMB.NA.landfalling.storms.100km.buffer.pts.nc\"\n",
    "track_file3 = \"100km_analysis/RCP45.COMB.NA.landfalling.storms.100km.buffer.pts.nc\"\n",
    "track_file4 = \"100km_analysis/RCP85.COMB.NA.landfalling.storms.100km.buffer.pts.nc\"\n",
    "\n",
    "track_file5 = \"200km_analysis/IBTrACS.NA.v04r00.landfalling.storms.200km.buffer.pts.nc\"\n",
    "track_file6 = \"200km_analysis/REF.COMB.NA.landfalling.storms.200km.buffer.pts.nc\"\n",
    "track_file7 = \"200km_analysis/RCP45.COMB.NA.landfalling.storms.200km.buffer.pts.nc\"\n",
    "track_file8 = \"200km_analysis/RCP85.COMB.NA.landfalling.storms.200km.buffer.pts.nc\"\n",
    "\n",
    "track_file9 = \"300km_analysis/IBTrACS.NA.v04r00.landfalling.storms.300km.buffer.pts.nc\"\n",
    "track_file10 = \"300km_analysis/REF.COMB.NA.landfalling.storms.300km.buffer.pts.nc\"\n",
    "track_file11 = \"300km_analysis/RCP45.COMB.NA.landfalling.storms.300km.buffer.pts.nc\"\n",
    "track_file12 = \"300km_analysis/RCP85.COMB.NA.landfalling.storms.300km.buffer.pts.nc\"\n",
    "\n",
    "file_list = [[track_file1, track_file2, track_file3, track_file4],     #create list of track files\n",
    "             [track_file5, track_file6, track_file7, track_file8],\n",
    "             [track_file9, track_file10, track_file11, track_file12]]\n",
    "\n",
    "file_array = np.asarray(file_list)               #convert list to array for better indexing\n",
    "\n",
    "ts_list = [[], [], []]                           #create empty lists to store data from track files\n",
    "\n",
    "file_groups = np.shape(file_array)[0]            #calculate number of file groups(buffers) and files per group\n",
    "files = np.shape(file_array)[1]\n",
    "\n",
    "for i in range(file_groups):                     #iterate through every track file buffer group\n",
    "    for j in range(files):                       #iterate through every track file in group\n",
    "        ts_dist = []                             #initialize ts distribution list\n",
    "        \n",
    "        DS = xr.open_dataset(file_array[i,j])    #open file and extract arrays\n",
    "        lons = DS.clon.values\n",
    "        lats = DS.clat.values\n",
    "        time = DS.time_str.values\n",
    "        DS.close()\n",
    "\n",
    "        nstorms = np.shape(lons)[0]  #get number of storms and times\n",
    "        ntimes = np.shape(lons)[1]\n",
    "\n",
    "        for k in range(nstorms):\n",
    "            lon_array = lons[k,:]    #get lons and lats for each storm\n",
    "            lat_array = lats[k,:]\n",
    "            lon_array = lon_array[lon_array < 0]  #only keep legitimate TC track points\n",
    "            lat_array = lat_array[lat_array > 0]\n",
    "    \n",
    "            if len(lon_array) != len(lat_array): #raise error if not all points have both a lon and lat coord\n",
    "                raise ValueError('lat/lon lengths are not equal at storm ' + str(i))\n",
    "              \n",
    "            for m in range(len(lon_array)):\n",
    "                \n",
    "                if m == len(lon_array)-1:        #avoid conflicts at last point \n",
    "                    continue\n",
    "                \n",
    "                if file_array[i,j].count('IBTrACS') == 1:\n",
    "                    time1 = np.datetime64(time[k,m].decode('UTF-8'))       \n",
    "                    time2 = np.datetime64(time[k,m+1].decode('UTF-8'))\n",
    "                    \n",
    "                if file_array[i,j].count('IBTrACS') == 0:\n",
    "                    time1_str = str(time[k,m])                #get initial strings in integer format\n",
    "                    time2_str = str(time[k,m+1])              #convert to np.datetime64 format\n",
    "                    time1_dt = time1_str[0:4] + '-' + time1_str[4:6] + '-' + time1_str[6:8] + ' ' + time1_str[8:10] + ':00:00'\n",
    "                    time2_dt = time2_str[0:4] + '-' + time2_str[4:6] + '-' + time2_str[6:8] + ' ' + time2_str[8:10] + ':00:00'\n",
    "                    time1 = np.datetime64(time1_dt)\n",
    "                    time2 = np.datetime64(time2_dt)\n",
    "                    \n",
    "                time_diff = np.timedelta64(time2-time1, 'h')  #get difference in times\n",
    "                expected_diff = np.timedelta64(6, 'h')        #set expected difference to 6 hours\n",
    "                hours = 6.0                                   #numerical value of expected time diff between pts\n",
    "                \n",
    "                if time_diff == expected_diff:                #all time differences should equal expected difference\n",
    "                    track_pt1 = (lon_array[m], lat_array[m])  #calculate translation speed in km/h for each point\n",
    "                    track_pt2 = (lon_array[m+1], lat_array[m+1])\n",
    "                    ts = get_distance(track_pt1, track_pt2) / hours \n",
    "                    ts_dist.append(ts)\n",
    "        \n",
    "        ts_list[i].append(ts_dist)     #append arrays into list\n",
    "\n",
    "ts_array = np.asarray(ts_list)         #convert to array and print result\n",
    "\n",
    "for i in range(file_groups):           #print medians\n",
    "    for j in range(files):\n",
    "        print(file_array[i,j] + \": \" + str(np.median(ts_array[i,j]).round(2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090f1ffe",
   "metadata": {},
   "source": [
    "### Program to combine 3 NetCDF files into a new file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8642abbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "\n",
    "track_file1 = \"200km_analysis/RCP85.NA.landfalling.storms.200km.buffer.pts.nc\"\n",
    "\n",
    "DS1 = xr.open_dataset(track_file1)\n",
    "lons1 = DS1.clon.values\n",
    "lats1 = DS1.clat.values\n",
    "max_w1 = DS1.vmax_2D.values\n",
    "time1 = DS1.time_str.values\n",
    "DS1.close()\n",
    "\n",
    "track_file2 = \"200km_analysis/RCP85002.NA.landfalling.storms.200km.buffer.pts.nc\"\n",
    "\n",
    "DS2 = xr.open_dataset(track_file2)\n",
    "lons2 = DS2.clon.values\n",
    "lats2 = DS2.clat.values\n",
    "max_w2 = DS2.vmax_2D.values\n",
    "time2 = DS2.time_str.values\n",
    "DS2.close()\n",
    "\n",
    "track_file3 = \"200km_analysis/RCP85003.NA.landfalling.storms.200km.buffer.pts.nc\"\n",
    "\n",
    "DS3 = xr.open_dataset(track_file3)\n",
    "lons3 = DS3.clon.values\n",
    "lats3 = DS3.clat.values\n",
    "max_w3 = DS3.vmax_2D.values\n",
    "time3 = DS3.time_str.values\n",
    "DS3.close()\n",
    "\n",
    "nstorms = [np.shape(lons1)[0], np.shape(lons2)[0], np.shape(lons3)[0]]\n",
    "ntimes = [np.shape(lons1)[1], np.shape(lons2)[1], np.shape(lons3)[1]]\n",
    "\n",
    "comb_lons = np.zeros((sum(nstorms), max(ntimes)))\n",
    "comb_lats = np.zeros((sum(nstorms), max(ntimes)))\n",
    "comb_maxw = np.zeros((sum(nstorms), max(ntimes)))\n",
    "comb_time = np.zeros((sum(nstorms), max(ntimes)), dtype = np.int32)\n",
    "\n",
    "n = 0\n",
    "for i in range(nstorms[0]):\n",
    "    for j in range(ntimes[0]):\n",
    "        comb_lons[n,j] = lons1[i,j]\n",
    "        comb_lats[n,j] = lats1[i,j]\n",
    "        comb_maxw[n,j] = max_w1[i,j] \n",
    "        comb_time[n,j] = time1[i,j]\n",
    "    n = n+1\n",
    "\n",
    "for i in range(nstorms[1]):\n",
    "    for j in range(ntimes[1]):\n",
    "        comb_lons[n,j] = lons2[i,j]\n",
    "        comb_lats[n,j] = lats2[i,j]\n",
    "        comb_maxw[n,j] = max_w2[i,j] \n",
    "        comb_time[n,j] = time2[i,j]\n",
    "    n = n+1\n",
    "\n",
    "for i in range(nstorms[2]):\n",
    "    for j in range(ntimes[2]):\n",
    "        comb_lons[n,j] = lons3[i,j]\n",
    "        comb_lats[n,j] = lats3[i,j]\n",
    "        comb_maxw[n,j] = max_w3[i,j] \n",
    "        comb_time[n,j] = time3[i,j]\n",
    "    n = n+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ab4416",
   "metadata": {},
   "source": [
    "Make a new NetCDF file from results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7072a3e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset>\n",
      "Dimensions:   (stormID: 191, time: 107)\n",
      "Dimensions without coordinates: stormID, time\n",
      "Data variables:\n",
      "    clon      (stormID, time) float64 -84.19 -80.92 -79.49 ... 0.0 0.0 0.0\n",
      "    clat      (stormID, time) float64 25.62 31.24 32.61 32.94 ... 0.0 0.0 0.0\n",
      "    vmax_2D   (stormID, time) float64 29.81 27.25 30.81 24.43 ... 0.0 0.0 0.0\n",
      "    time_str  (stormID, time) int32 2070020800 2070020812 2070020818 ... 0 0 0\n",
      "Attributes:\n",
      "    description:    NetCDF file that combines the buffer points from all 3 RC...\n",
      "    author:         Justin Willson\n",
      "    creation_date:  2021-07-28\n"
     ]
    }
   ],
   "source": [
    "todays_date = \"2021-07-28\"\n",
    "\n",
    "new_ds = xr.Dataset(\n",
    "        data_vars = dict(\n",
    "        clon = ([\"stormID\", \"time\"], comb_lons),\n",
    "        clat = ([\"stormID\", \"time\"], comb_lats),\n",
    "        vmax_2D = ([\"stormID\", \"time\"], comb_maxw),\n",
    "        time_str = ([\"stormID\", \"time\"], comb_time)\n",
    "        ),\n",
    "        attrs = dict(\n",
    "        description = \"NetCDF file that combines the buffer points from all 3 RCP85 ensembles.\",\n",
    "        author = \"Justin Willson\",\n",
    "        creation_date = todays_date\n",
    "        ),\n",
    ")\n",
    "\n",
    "print(new_ds)\n",
    "\n",
    "new_ds.to_netcdf(\"200km_analysis/RCP85.COMB.NA.landfalling.storms.200km.buffer.pts.nc\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
